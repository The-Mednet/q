---
# Prometheus Configuration for SMTP Relay Monitoring
# Comprehensive monitoring for medical-grade email infrastructure

apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: mednet-monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'mednet-email-cluster'
        environment: 'production'
    
    # Alertmanager configuration
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
    
    # Load alerting rules
    rule_files:
    - "/etc/prometheus/rules/*.yml"
    
    # Scrape configurations
    scrape_configs:
    # SMTP Relay Service metrics
    - job_name: 'smtp-relay'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - mednet-email
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: smtp-relay
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'go_.*'
        action: drop  # Drop Go runtime metrics to reduce noise
      scrape_interval: 10s
      scrape_timeout: 5s
    
    # MySQL metrics
    - job_name: 'mysql'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - mednet-email
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: mysql
      - source_labels: [__meta_kubernetes_pod_container_name]
        action: keep
        regex: mysql-exporter
      - source_labels: [__meta_kubernetes_pod_container_port_name]
        action: keep
        regex: metrics
      scrape_interval: 30s
      scrape_timeout: 10s
    
    # Redis metrics
    - job_name: 'redis'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
          - mednet-email
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: redis
      - source_labels: [__meta_kubernetes_pod_container_name]
        action: keep
        regex: redis-exporter
      - source_labels: [__meta_kubernetes_pod_container_port_name]
        action: keep
        regex: metrics
      scrape_interval: 30s
    
    # Kubernetes cluster metrics
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      scrape_interval: 30s
    
    # Kubernetes pods
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    # cAdvisor for container metrics
    - job_name: 'kubernetes-cadvisor'
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
      metric_relabel_configs:
      # Only keep container metrics for our services
      - source_labels: [container_label_io_kubernetes_pod_namespace]
        regex: 'mednet-email|mednet-monitoring'
        action: keep
    
    # External service monitoring
    - job_name: 'blackbox-email-endpoints'
      metrics_path: /probe
      params:
        module: [smtp_starttls]
      static_configs:
      - targets:
        - smtp-relay-service.mednet-email.svc.cluster.local:2525
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
    
    # Gmail API monitoring
    - job_name: 'gmail-api-health'
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
      - targets:
        - https://www.googleapis.com/oauth2/v1/tokeninfo
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
      scrape_interval: 60s

---
# Prometheus Alerting Rules for SMTP Relay
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: mednet-monitoring
data:
  smtp-relay.yml: |
    groups:
    - name: smtp-relay-alerts
      interval: 30s
      rules:
      
      # Critical Alerts (P1 - Immediate Response Required)
      - alert: SMTPRelayDown
        expr: up{job="smtp-relay"} == 0
        for: 1m
        labels:
          severity: critical
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "SMTP Relay service is down"
          description: "SMTP Relay service in namespace {{ $labels.kubernetes_namespace }} has been down for more than 1 minute. This affects all email delivery for medical services."
          runbook_url: "https://docs.mednet.com/runbooks/smtp-relay-down"
      
      - alert: EmailQueueBacklog
        expr: email_queue_depth > 1000
        for: 5m
        labels:
          severity: critical
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "Email queue backlog is critically high"
          description: "Email queue has {{ $value }} messages pending for more than 5 minutes. This may indicate processing issues or API rate limiting."
          runbook_url: "https://docs.mednet.com/runbooks/email-queue-backlog"
      
      - alert: EmailProcessingFailureRate
        expr: rate(emails_failed_total[5m]) / rate(emails_processed_total[5m]) > 0.1
        for: 3m
        labels:
          severity: critical
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "High email processing failure rate"
          description: "Email failure rate is {{ $value | humanizePercentage }} over the last 5 minutes. Medical notifications may not be delivered."
      
      - alert: DatabaseConnectionFailure
        expr: mysql_up == 0
        for: 2m
        labels:
          severity: critical
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "MySQL database is unreachable"
          description: "MySQL database connection has failed for more than 2 minutes. Email queue persistence is affected."
      
      # High Alerts (P2 - Response within 30 minutes)
      - alert: SMTPRelayHighMemoryUsage
        expr: container_memory_usage_bytes{pod=~"smtp-relay-.*"} / container_spec_memory_limit_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "SMTP Relay pod high memory usage"
          description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit for more than 5 minutes."
      
      - alert: SMTPRelayHighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{pod=~"smtp-relay-.*"}[5m]) / container_spec_cpu_quota * 100 > 80
        for: 10m
        labels:
          severity: warning
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "SMTP Relay pod high CPU usage"
          description: "Pod {{ $labels.pod }} CPU usage is {{ $value }}% for more than 10 minutes."
      
      - alert: EmailRateLimitApproaching
        expr: (emails_sent_today / email_daily_rate_limit) > 0.8
        for: 1m
        labels:
          severity: warning
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "Email rate limit approaching"
          description: "Daily email rate limit is {{ $value | humanizePercentage }} consumed. Consider increasing limits or investigating high volume."
      
      - alert: SlowEmailProcessing
        expr: rate(emails_processed_total[5m]) < 5
        for: 10m
        labels:
          severity: warning
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "Email processing rate is slow"
          description: "Email processing rate is {{ $value }} emails/second, which is below normal threshold."
      
      # Medium Alerts (P3 - Response within 2 hours)
      - alert: EmailRetryRateHigh
        expr: rate(emails_retried_total[10m]) > 2
        for: 15m
        labels:
          severity: info
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "High email retry rate detected"
          description: "Email retry rate is {{ $value }} retries/second, indicating potential delivery issues."
      
      - alert: RedisConnectionIssues
        expr: redis_up == 0
        for: 5m
        labels:
          severity: warning
          service: smtp-relay
          team: mednet-infrastructure
        annotations:
          summary: "Redis connection issues"
          description: "Redis connection has failed. Distributed rate limiting may be affected."
      
      # Database-specific alerts
      - alert: MySQLSlowQueries
        expr: rate(mysql_global_status_slow_queries[5m]) > 2
        for: 10m
        labels:
          severity: warning
          service: mysql
          team: mednet-infrastructure
        annotations:
          summary: "High rate of MySQL slow queries"
          description: "MySQL slow query rate is {{ $value }} queries/second."
      
      - alert: MySQLConnectionsHigh
        expr: mysql_global_status_threads_connected / mysql_global_variables_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
          service: mysql
          team: mednet-infrastructure
        annotations:
          summary: "MySQL connection usage is high"
          description: "MySQL is using {{ $value | humanizePercentage }} of available connections."
      
      # External API monitoring
      - alert: GmailAPIDown
        expr: probe_success{job="gmail-api-health"} == 0
        for: 5m
        labels:
          severity: critical
          service: gmail-api
          team: mednet-infrastructure
        annotations:
          summary: "Gmail API is unreachable"
          description: "Gmail API health check has failed for more than 5 minutes. Email delivery via Gmail is affected."
      
      # Kubernetes resource alerts
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total{namespace="mednet-email"}[15m]) > 0
        for: 5m
        labels:
          severity: warning
          service: "{{ $labels.pod }}"
          team: mednet-infrastructure
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently."
      
      - alert: NodeDiskSpaceHigh
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.1
        for: 5m
        labels:
          severity: critical
          team: mednet-infrastructure
        annotations:
          summary: "Node disk space critically low"
          description: "Node {{ $labels.instance }} has less than 10% disk space remaining."

  recording-rules.yml: |
    groups:
    - name: smtp-relay-recording-rules
      interval: 30s
      rules:
      
      # Email processing rates
      - record: emails_processed_per_second
        expr: rate(emails_processed_total[1m])
      
      - record: emails_failed_per_second
        expr: rate(emails_failed_total[1m])
      
      - record: email_failure_rate
        expr: rate(emails_failed_total[5m]) / rate(emails_processed_total[5m])
      
      # Queue metrics
      - record: email_queue_depth_avg
        expr: avg(email_queue_depth)
      
      - record: email_processing_latency_p95
        expr: histogram_quantile(0.95, rate(email_processing_duration_seconds_bucket[5m]))
      
      # Resource utilization
      - record: smtp_relay_memory_utilization
        expr: container_memory_usage_bytes{pod=~"smtp-relay-.*"} / container_spec_memory_limit_bytes
      
      - record: smtp_relay_cpu_utilization
        expr: rate(container_cpu_usage_seconds_total{pod=~"smtp-relay-.*"}[5m]) / container_spec_cpu_quota
      
      # Daily rate limit tracking
      - record: emails_sent_today_rate
        expr: emails_sent_today / email_daily_rate_limit
      
      # Workspace-specific metrics
      - record: emails_per_workspace_per_second
        expr: rate(emails_processed_total[1m]) by (workspace_id)
      
      # Database performance
      - record: mysql_connection_utilization
        expr: mysql_global_status_threads_connected / mysql_global_variables_max_connections
      
      - record: mysql_query_rate
        expr: rate(mysql_global_status_questions[1m])

---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: smtp-relay-dashboard
  namespace: mednet-monitoring
data:
  smtp-relay-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "SMTP Relay Service - Medical Email Infrastructure",
        "tags": ["smtp", "email", "medical", "infrastructure"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Email Processing Overview",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(emails_processed_total[5m]))",
                "legendFormat": "Emails/sec"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "thresholds"
                },
                "thresholds": {
                  "steps": [
                    {"color": "red", "value": 0},
                    {"color": "yellow", "value": 5},
                    {"color": "green", "value": 10}
                  ]
                }
              }
            },
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Queue Depth",
            "type": "graph",
            "targets": [
              {
                "expr": "email_queue_depth",
                "legendFormat": "{{ instance }}"
              }
            ],
            "yAxes": [
              {
                "label": "Messages",
                "min": 0
              }
            ],
            "alert": {
              "conditions": [
                {
                  "evaluator": {
                    "params": [1000],
                    "type": "gt"
                  },
                  "operator": {
                    "type": "and"
                  },
                  "query": {
                    "params": ["A", "5m", "now"]
                  },
                  "reducer": {
                    "params": [],
                    "type": "avg"
                  },
                  "type": "query"
                }
              ],
              "executionErrorState": "alerting",
              "for": "5m",
              "frequency": "10s",
              "handler": 1,
              "name": "Queue Depth Alert",
              "noDataState": "no_data",
              "notifications": []
            },
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "30s"
      }
    }